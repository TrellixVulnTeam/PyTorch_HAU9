{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as f\n",
    "import csv \n",
    "import random\n",
    "import re\n",
    "import os\n",
    "import unicodedata\n",
    "import codecs\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if CUDA else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines_filepath = os.path.join(\"cornell movie-dialogs corpus\",\"movie_lines.txt\")\n",
    "conv_filepath = os.path.join(\"cornell movie-dialogs corpus\",\"movie_conversations.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some lines\n",
    "with open(lines_filepath,'r',encoding=\"iso-8859-1\") as file:\n",
    "    lines = file.readlines()\n",
    "    for line in lines[:8]:\n",
    "        #print(line.strip())\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting each line of the file into a dictioanry of fields (LineId,CharacterId,MovieId,Character,Text)\n",
    "line_fields = [\"lineId\",\"characterId\",\"movieId\",\"character\",\"text\"]\n",
    "lines = {}\n",
    "with open(lines_filepath,'r',encoding=\"iso-8859-1\") as f:\n",
    "    for line in f:\n",
    "        values = line.split(\" +++$+++ \")\n",
    "        # Extract fileds\n",
    "        lineobj = {}\n",
    "        for i,field in enumerate(line_fields):\n",
    "            lineobj[field] = values[i]\n",
    "            #print(lineobj)\n",
    "        lines[lineobj['lineId']] = lineobj\n",
    "        #print(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Groups fields of lines from 'Loadlines' into conversations based on \"movie_conversations.txt\"\n",
    "conv_fields = [\"character1Id\",\"character2Id\",\"movieId\",\"utterenceIDs\"]\n",
    "conversations = []\n",
    "with open(conv_filepath,'r',encoding='iso-8859-1') as f:\n",
    "    for line in f:\n",
    "        values = line.split(\" +++$+++ \")\n",
    "        # Extract Fields\n",
    "        convObj = {}\n",
    "        for i,field in enumerate(conv_fields):\n",
    "            convObj[field] = values[i]\n",
    "        # Convert string result from split to list, since convObj[\"utterenceIDs\"] == \"[\"L598765\",\"L567890\",\"....\"]\"\n",
    "        lineIds = eval(convObj[\"utterenceIDs\"])\n",
    "        #print(lineIds)\n",
    "        # Reassemble lines\n",
    "        convObj[\"lines\"] = []\n",
    "        for lineId in lineIds:\n",
    "            convObj[\"lines\"].append(lines[lineId])\n",
    "        conversations.append(convObj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract pairs of sentences from conversations\n",
    "qa_pairs = []\n",
    "for conversation in conversations:\n",
    "    # Iterate over all the lines of conversation\n",
    "    for i in range(len(conversation[\"lines\"]) - 1):\n",
    "        inputLine = conversation[\"lines\"][i][\"text\"].strip()\n",
    "        targetLine = conversation[\"lines\"][i+1][\"text\"].strip()\n",
    "        # Filter wrong samples if one of the lists is empty\n",
    "        if inputLine and targetLine:\n",
    "            qa_pairs.append([inputLine,targetLine])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " writing newly formatted file\n",
      "Done writing to the file \n"
     ]
    }
   ],
   "source": [
    "# Define path to new file\n",
    "datafile = os.path.join(\"cornell movie-dialogs corpus\",\"formatted_movie_lines.txt\")\n",
    "delimiter = '\\t'\n",
    "# Unescape the delimiter\n",
    "delimiter = str(codecs.decode(delimiter,\"unicode_escape\"))\n",
    "\n",
    "# write new csv file\n",
    "print(\"\\n writing newly formatted file\")\n",
    "with open(datafile,\"w\",encoding='utf-8') as outputfile:\n",
    "    writer = csv.writer(outputfile,delimiter=delimiter)\n",
    "    for pair in qa_pairs:\n",
    "        #print(pair[0])\n",
    "        writer.writerow(pair)\n",
    "print(\"Done writing to the file \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.\tWell, I thought we'd start with pronunciation, if that's okay with you.\n",
      "\n",
      "Well, I thought we'd start with pronunciation, if that's okay with you.\tNot the hacking and gagging and spitting part.  Please.\n",
      "\n",
      "Not the hacking and gagging and spitting part.  Please.\tOkay... then how 'bout we try out some French cuisine.  Saturday?  Night?\n",
      "\n",
      "You're asking me out.  That's so cute. What's your name again?\tForget it.\n",
      "\n",
      "No, no, it's my fault -- we didn't have a proper introduction ---\tCameron.\n",
      "\n",
      "Cameron.\tThe thing is, Cameron -- I'm at the mercy of a particularly hideous breed of loser.  My sister.  I can't date until she does.\n",
      "\n",
      "The thing is, Cameron -- I'm at the mercy of a particularly hideous breed of loser.  My sister.  I can't date until she does.\tSeems like she could get a date easy enough...\n",
      "\n",
      "Why?\tUnsolved mystery.  She used to be really popular when she started high school, then it was just like she got sick of it or something.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Visualize some Lines\n",
    "datafile = os.path.join(\"cornell movie-dialogs corpus\",\"formatted_movie_lines.txt\")\n",
    "with open (datafile,'r',encoding=\"iso-8859-1\") as file:\n",
    "    lines = file.readlines()\n",
    "    for line in lines[:8]:\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_TOKEN = 0\n",
    "SOS_TOKEN = 0\n",
    "EOS_TOKEN = 2\n",
    "\n",
    "class Vocabulary:\n",
    "    def __init__(self,name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {PAD_TOKEN:\"PAD\",SOS_TOKEN:\"SOS\",EOS_TOKEN:\"EOS\"}\n",
    "        self.num_words = 3\n",
    "        \n",
    "    def addSentence(self,sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "            \n",
    "    def addWord(self,word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.num_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.num_words] = word\n",
    "            self.num_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "            \n",
    "    # Remove words below a certain count threshold\n",
    "    def trim(self, min_count):\n",
    "        keep_words = []\n",
    "        for k,v in self.word2count.items():\n",
    "            if v >= min_count:\n",
    "                keep_words.append(k)\n",
    "                \n",
    "        print('keep_words {}/{} = {:.4f}'.format(len(keep_words),len(self.word2index),len(keep_words)/len(self.word2index)))\n",
    "    \n",
    "        # Reinitialize the dictionaries \n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {PAD_TOKEN:\"PAD\",SOS_TOKEN:\"SOS\",EOS_TOKEN:\"EOS\"}\n",
    "        self.num_words = 3 \n",
    "        \n",
    "        for word in keep_words:\n",
    "              self.addWord(word)\n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn a unicode string to plain ASCII\n",
    "import unicodedata\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD',s) if unicodedata.category(c) != 'Mn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hel'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join(['h','e','l'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Adios,Pequeno....'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unicodeToAscii(\"Adiós,Pequeño....\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lowercase, trim white spaces, lines... etc, and remove non letters character.\n",
    "import re\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    # Replace any .!? by a whitespace  + the character --> '!' = ' !'. \\1 means that first bracketed group -->[,!] .\n",
    "    # r is to not consider \\1 as a character (r to escape a backslash). + means one or more\n",
    "    s = re.sub(r\"([.!?])\",r\" \\1\",s)\n",
    "    # Remove character that is not a sequence of lower or uppercase\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\",r\" \",s)\n",
    "    # Remove a sequence of whitespace character\n",
    "    s = re.sub(r\"\\s+\",r\" \",s).strip()\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'aaa !s s dd ?'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalizeString(\"aaa123!s's   dd?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading and Processing the file........... please wait\n",
      "Done Reading !!!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "datafile = os.path.join(\"cornell movie-dialogs corpus\",\"formatted_movie_lines.txt\")\n",
    "# Read the files and split into lines\n",
    "print(\"Reading and Processing the file........... please wait\")\n",
    "lines = open(datafile,encoding=\"iso-8859-1\").read().strip().split('\\n')\n",
    "# Split every line into pairs and normalize\n",
    "pairs = [[normalizeString(s) for s in pair.split('\\t')] for pair in lines]\n",
    "print(\"Done Reading !!!\")\n",
    "voc = Vocabulary(\"cornell movie-dialogs corpus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.',\n",
       " \"Well, I thought we'd start with pronunciation, if that's okay with you.\"]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines[0].split('\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns true if both sentences in a pair 'p' are under the max length threshold\n",
    "MAX_LENGTH = 10 # Maximum sentence length to consider\n",
    "def filterPair(p):\n",
    "    # Input sequences need to preserve the last word for EOS token\n",
    "    return len(p[0].split()) < MAX_LENGTH and len(p[1].split()) < MAX_LENGTH\n",
    "\n",
    "# Filter pairs using filter pair condition\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 221282 pairs/conversations in the dataset \n",
      "After filtering there are 64243 pairs/conversations \n"
     ]
    }
   ],
   "source": [
    "pairs = [pair for pair in pairs if len(pair) > 1]\n",
    "print(\"There are {} pairs/conversations in the dataset \".format(len(pairs)))\n",
    "pairs = filterPairs(pairs)\n",
    "print(\"After filtering there are {} pairs/conversations \".format(len(pairs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counted words : 18109\n",
      "['there .', 'where ?']\n",
      "['you have my word . as a gentleman', 'you re sweet .']\n",
      "['hi .', 'looks like things worked out tonight huh ?']\n",
      "['you know chastity ?', 'i believe we share an art instructor']\n",
      "['have fun tonight ?', 'tons']\n",
      "['well no . . .', 'then that s all you had to say .']\n",
      "['then that s all you had to say .', 'but']\n",
      "['but', 'you always been this selfish ?']\n",
      "['do you listen to this crap ?', 'what crap ?']\n",
      "['what good stuff ?', 'the real you .']\n"
     ]
    }
   ],
   "source": [
    "# Loop through each pair and add the question and reply e=sentence to the vocabulary\n",
    "for pair in pairs:\n",
    "    voc.addSentence(pair[0])\n",
    "    voc.addSentence(pair[1])\n",
    "print(\"counted words :\", voc.num_words)\n",
    "for pair in pairs[:10]:\n",
    "    print(pair)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keep_words 7840/18106 = 0.4330\n",
      "Trimmed from 64243 pairs to 57971,0.9024 of total \n"
     ]
    }
   ],
   "source": [
    "MIN_COUNT = 3   # minimum word count threshold for trimming\n",
    "def trimRareWords(voc,pairs,MIN_COUNT):\n",
    "    # Trim words used under the MIN_COUNT from the voc\n",
    "    voc.trim(MIN_COUNT)\n",
    "    # filter out paired with trimmed words\n",
    "    keep_pairs = []\n",
    "    for pair in pairs:\n",
    "        input_sentence = pair[0]\n",
    "        output_sentence = pair[1]\n",
    "        keep_input = True\n",
    "        keep_ourput = True\n",
    "        \n",
    "        # Check your sentence \n",
    "        for word in input_sentence.split(' '):\n",
    "            if word not in voc.word2index:\n",
    "                keep_input = False\n",
    "                break\n",
    "                \n",
    "        # Check output sentence\n",
    "        for word in output_sentence.split(' '):\n",
    "            if word not in voc.word2index:\n",
    "                keep_output = False\n",
    "                break\n",
    "                \n",
    "        # only keep pairs that do not contain trimmed word(s) in their input or output sentence\n",
    "        if keep_input and keep_ourput:\n",
    "            keep_pairs.append(pair)\n",
    "            \n",
    "    print(\"Trimmed from {} pairs to {},{:.4f} of total \".format(len(pairs),len(keep_pairs),len(keep_pairs)/len(pairs)))\n",
    "    return keep_pairs\n",
    "\n",
    "# Print voc and pairs\n",
    "pairs = trimRareWords(voc,pairs,MIN_COUNT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexFromSentence(voc,sentence):\n",
    "    return [voc.word2index[word] for word in sentence.split(' ')] + [EOS_TOKEN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'you have my word . as a gentleman'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7, 8, 9, 10, 4, 11, 12, 13, 2]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the function\n",
    "indexFromSentence(voc,pairs[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['there .', 'you have my word . as a gentleman', 'hi .', 'have fun tonight ?', 'well no . . .', 'then that s all you had to say .', 'but', 'do you listen to this crap ?', 'what good stuff ?', 'the real you .']\n",
      "10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[3, 4, 2],\n",
       " [7, 8, 9, 10, 4, 11, 12, 13, 2],\n",
       " [16, 4, 2],\n",
       " [8, 31, 22, 6, 2],\n",
       " [33, 34, 4, 4, 4, 2],\n",
       " [35, 36, 37, 38, 7, 39, 40, 41, 4, 2],\n",
       " [42, 2],\n",
       " [47, 7, 48, 40, 45, 49, 6, 2],\n",
       " [50, 51, 52, 6, 2],\n",
       " [53, 54, 7, 4, 2]]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define some samples for output\n",
    "inp = []\n",
    "out = []\n",
    "for pair in pairs[:10]:\n",
    "    inp.append(pair[0])\n",
    "    out.append(pair[1])\n",
    "print(inp)\n",
    "print(len(inp))\n",
    "indexes = [indexFromSentence(voc,sentence) for sentence in inp]\n",
    "indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zeroPadding(l,fillvalue=0):\n",
    "    return list(itertools.zip_longest(*l,fillvalue=fillvalue))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leng = [len(ind) for ind in indexes]\n",
    "max(leng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(3, 7, 16, 8, 33, 35, 42, 47, 50, 53),\n",
       " (4, 8, 4, 31, 34, 36, 2, 7, 51, 54),\n",
       " (2, 9, 2, 22, 4, 37, 0, 48, 52, 7),\n",
       " (0, 10, 0, 6, 4, 38, 0, 40, 6, 4),\n",
       " (0, 4, 0, 2, 4, 7, 0, 45, 2, 2),\n",
       " (0, 11, 0, 0, 2, 39, 0, 49, 0, 0),\n",
       " (0, 12, 0, 0, 0, 40, 0, 6, 0, 0),\n",
       " (0, 13, 0, 0, 0, 41, 0, 2, 0, 0),\n",
       " (0, 2, 0, 0, 0, 4, 0, 0, 0, 0),\n",
       " (0, 0, 0, 0, 0, 2, 0, 0, 0, 0)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the function\n",
    "test_result = zeroPadding(indexes)\n",
    "print(len(test_result)) # The max length is now the number of rows or the batch size\n",
    "test_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binaryMatrix(l,value=0):\n",
    "    m = []\n",
    "    for i,seq in enumerate(l):\n",
    "        m.append([])\n",
    "        for token in seq:\n",
    "            if token in seq:\n",
    "                if token == PAD_TOKEN:\n",
    "                    m[i].append(0)\n",
    "                else:\n",
    "                    m[i].append(1)\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       " [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       " [1, 1, 1, 1, 1, 1, 0, 1, 1, 1],\n",
       " [0, 1, 0, 1, 1, 1, 0, 1, 1, 1],\n",
       " [0, 1, 0, 1, 1, 1, 0, 1, 1, 1],\n",
       " [0, 1, 0, 0, 1, 1, 0, 1, 0, 0],\n",
       " [0, 1, 0, 0, 0, 1, 0, 1, 0, 0],\n",
       " [0, 1, 0, 0, 0, 1, 0, 1, 0, 0],\n",
       " [0, 1, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_result = binaryMatrix(test_result)\n",
    "binary_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns padded input sequence tensor and as well as a tensor of length for each of the sequence in batch\n",
    "def inputVar(l,voc):\n",
    "    indexes_batch = [indexFromSentence(voc,sentence) for sentence in l]\n",
    "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
    "    padList = zeroPadding(indexes_batch)\n",
    "    padVar = torch.LongTensor(padList)\n",
    "    return padVar,lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns padded target sequence tensor, padding mask and max target length\n",
    "def outputVar(l,voc):\n",
    "    indexes_batch = [indexFromSentence(voc,sentence) for sentence in l]\n",
    "    max_target_len = max([len(indexes) for indexes in indexes_batch])\n",
    "    padList = zeroPadding(indexes_batch)\n",
    "    mask = binaryMatrix(padList)\n",
    "    mask = [torch.ByteTensor(mask)]\n",
    "    padVar = torch.LongTensor(padList)\n",
    "    return padVar,mask,max_target_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns all items for a given batch of pairs\n",
    "def batch2TrainData(voc,pair_batch):\n",
    "    # sort the question in descending length\n",
    "    pair_batch.sort(key=lambda x: len(x[0].split(\" \")),reverse=True)\n",
    "    input_batch,output_batch = [],[]\n",
    "    for pair in pair_batch:\n",
    "        input_batch.append(pair[0])\n",
    "        output_batch.append(pair[1])\n",
    "    inp,lengths = inputVar(input_batch,voc) \n",
    "    output,mask,max_target_len = outputVar(output_batch,voc)\n",
    "    return inp,lengths,output,mask,max_target_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input_variable :\n",
      "tensor([[ 122,    7,  100,   45,   25],\n",
      "        [  50,   80,  197, 1433,  197],\n",
      "        [  92,   47,  117, 2232,  117],\n",
      "        [  27,   36,    7,   29,   26],\n",
      "        [ 123,  293,  112, 2884,   76],\n",
      "        [  40,  117, 1734,   66,   66],\n",
      "        [  47,    7,  276,    2,    2],\n",
      "        [  95,    6,    2,    0,    0],\n",
      "        [   6,    2,    0,    0,    0],\n",
      "        [   2,    0,    0,    0,    0]])\n",
      "lengths tensor([10,  9,  8,  7,  7])\n",
      "target_variable :\n",
      "tensor([[  27,    7,   50,   53,   25],\n",
      "        [ 132,   82,   47, 1433,  534],\n",
      "        [ 307,   53,    7,  115, 1939],\n",
      "        [ 637,  121,  280, 7155, 5749],\n",
      "        [   4,  682,   50, 1206,    4],\n",
      "        [   2,    4,  101, 4160,    2],\n",
      "        [   0, 7636,  405,  756,    0],\n",
      "        [   0,    4,    6,    4,    0],\n",
      "        [   0,    2,    2,    2,    0]])\n",
      "mask :\n",
      "[tensor([[1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1],\n",
      "        [0, 1, 1, 1, 0],\n",
      "        [0, 1, 1, 1, 0],\n",
      "        [0, 1, 1, 1, 0]], dtype=torch.uint8)]\n",
      "max_target_len 9\n"
     ]
    }
   ],
   "source": [
    "# Example for validation\n",
    "small_batch_size = 5\n",
    "batches = batch2TrainData(voc,[random.choice(pairs) for _ in range(small_batch_size)])\n",
    "input_variable,lengths,target_variable,mask,max_target_len = batches\n",
    "\n",
    "print(\"Input_variable :\")\n",
    "print(input_variable)\n",
    "print(\"lengths\",lengths)\n",
    "print(\"target_variable :\")\n",
    "print(target_variable)\n",
    "print(\"mask :\")\n",
    "print(mask)\n",
    "print(\"max_target_len\",max_target_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self,hidden_size,embedding,n_layers=1,dropout=0):\n",
    "        super(EncoderRNN,self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = embedding\n",
    "        # Initialize GRU; the input_size and hidden_size params are bith set to 'hidden_size'\n",
    "        # because our input size is a word embedding with numner of features == hidden_size\n",
    "        self.gru = nn.GRU(hidden_size,hidden_size,n_layers,dropout=(0 if n_layers == 1 else dropout),bidirectional=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self,input_seq,input_lengths,hidden=None):\n",
    "    # input_seq : batch of input sentences; shape=(max_length,batch_size)\n",
    "    # input_lengths : List of sentence Lengths corresponding to each sentence in a batch\n",
    "    # hidden state, of shape: (n_layers x num_directions,batch_size,hidden_size)\n",
    "    # convert word indexes to embeddings\n",
    "    embedded = self.embedding(input_seq)\n",
    "    # pack padded batch of sequences for RNN module\n",
    "    packed = torch.nn.utils.rnn.pack_added_sequence(embedded,input_lengths)\n",
    "    # Forward pass through GRU\n",
    "    outputs,hidden = self.gru(packed,hidden)\n",
    "    # unpack padding\n",
    "    outputs,_ = torch.nn.utils.rnn.pad_packed_sequence(outputs)\n",
    "    # Sum bidirectional GRU outputs\n",
    "    outputs = outputs[:,:,:self.hidden_size] + outputs[:,:,self.hidden_size]\n",
    "    # Return output and final hidden state\n",
    "    # outputs: the output features h_t from the last Layer of the GRU, for each timestep (sum of bidirectional outputs)\n",
    "    # outputs shape=(max_length,batch_size,hidden_size)\n",
    "    # hidden: hidden state for the last timestep , of shape=(n_layers x num_directions,batch_size,hidden_size)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.0711, -0.1904, -0.1600],\n",
       "         [ 0.1100, -0.1772,  1.7750],\n",
       "         [-0.1180, -1.1073, -0.0408],\n",
       "         [-0.3518,  2.1390,  0.8877]],\n",
       "\n",
       "        [[ 0.7728, -0.7698, -2.5957],\n",
       "         [-0.3379,  0.1374,  0.2124],\n",
       "         [-0.6203,  0.3695,  0.0169],\n",
       "         [ 0.5683, -1.5721,  1.0113]],\n",
       "\n",
       "        [[ 0.2044,  0.1210, -1.0629],\n",
       "         [ 0.4517, -0.5212, -0.2448],\n",
       "         [ 0.9672, -0.2963, -0.0935],\n",
       "         [-0.8362,  0.8063,  1.1282]],\n",
       "\n",
       "        [[-0.0656,  0.8922,  0.6014],\n",
       "         [-0.2775,  0.9626,  1.8764],\n",
       "         [-0.2121,  0.3946,  1.5698],\n",
       "         [-1.1360,  0.3262,  0.6073]],\n",
       "\n",
       "        [[ 1.0015, -2.3042,  0.1872],\n",
       "         [ 0.1053, -0.2224,  0.5694],\n",
       "         [ 0.1727, -1.6166,  0.9791],\n",
       "         [ 1.4554, -1.1486, -0.1381]]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.randn(5,4,3)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Luong Attention Layer\n",
    "class Attn(torch.nn.Module):\n",
    "    def __init__(self,method,hidden_size):\n",
    "        super(Attn,self).__init__()\n",
    "        self.method = method\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "    def dot_score(self,hidden,encoder_output):\n",
    "        # Element-wise multiply the current target decoder state with the encoder output and sum them\n",
    "        return torch.sum(hidden * encoder_output , dim=2)\n",
    "    \n",
    "    def forward(self,hidden,encoder_outputs):\n",
    "        # hidden of shape: (1,batch_size,hidden_size)\n",
    "        # encoder_outputs of shape: (max_length,batch_size,hidden_size)\n",
    "        \n",
    "        # calculate the attention weights (energies)\n",
    "        attn_energies = self.dot_score(hidden,encoder_outputs) # (max_length,batch_size)\n",
    "        # Transpose max_length and batch_size dimensions\n",
    "        attn_energies = attn_energies.t() # (batch_size,max_length)\n",
    "        # Return the softmax normalized probability scores \n",
    "        return F.softmax(attn_energies,dim=1).unsqueeze(1) # (batch_size,1,max_length)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
